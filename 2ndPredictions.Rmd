---
title: "Second Predictive Analysis"
author: "Michael Harrison"
date: "10/10/2017"
output: rmarkdown::github_document
---

# Document Description
This markdown document contains the second predictive analysis of the data prepared by the functions in this folder. The last study utilized a limited set of data due to 

The aim is to predict the total stops of a production run given the breakdown of materials. 

# Data Preparation
## Load Packages and Functions
```{r}
# Load necessary package; source function to return table for analysis
library(caret)
source("productionBreakdown.R")
```
## Prepare Data
```{r}
# Data contains NAs generated from merging tables; removes NA rows
data <- na.omit(productionBreakdown())
# Removing row identifier (millstyle), and date
data <- subset(data, select = -c(millstyle, date))
```
## Partition Data for Training, Validation, & Testing
```{r}
#Establishes constant seed for consequent use in model development
seed <- 7

# Partition data to create training, validation, and testing sets;
# First split: 60/40 training/validation_testing
set.seed(seed)
inVal <- createDataPartition(data$totalstops,
                             p = .6,
                             list = FALSE)
val_test <- data[-inVal,]
training <- data[inVal,]

# Second split: 50/50 validation/testing
set.seed(seed)
inTrain <- createDataPartition(val_test$totalstops, 
                               p = .5,
                               list = FALSE)
validation <- val_test[inTrain,]
testing <- val_test[-inTrain,]
```

### Data set dimensions
```{r}
dataset.dimensions <- list(Training = dim(training), 
                          Validation = dim(validation), 
                          Testing = dim(testing))
dataset.dimensions
```


# Baseline Model Fit
## Baseline Models Training
```{r}
# Set training control parameters for repeated cross validation and evaluation metric
trainControl <- trainControl(method = "repeatedCV",
                             number = 10,
                             repeats = 3,
                             allowParallel = TRUE)
metric <- "RMSE"

# Loads Parallel processing libraries
library(parallel)
library(doParallel)

# Initializes Parallel Processing
cluster <- makeCluster(detectCores()-1)
registerDoParallel(cluster)

# SVM
set.seed(seed)
fit.svm <- train(totalstops~.,
                 data = training,
                 method = "svmRadial",
                 metric = metric,
                 preProc=c("center", "scale"),
                 trControl = trainControl)

# XGBLinear
set.seed(seed)
fit.xgbLinear <- train(totalstops~.,
                 data = training,
                 method = "xgbLinear",
                 metric = metric,
                 preProc=c("center", "scale"),
                 trControl = trainControl)

# Random Forest
set.seed(seed)
fit.rf <- train(totalstops~.,
                data = training,
                method = "rf",
                metric = metric,
                preProc=c("center", "scale"),
                trControl = trainControl)

# Stops Parallel Processing
stopCluster(cluster)
registerDoSEQ()

# Results
fit.results <- resamples(list(SVM=fit.svm,
                              RF=fit.rf,
                              XGBLin = fit.xgbLinear))
```

## Baseline Model Results
```{r}
dotplot(fit.results)
```

```{r}
summary(fit.results)
```

# Baseline Model Validation
## Data PreProcessing and Training Parameters
```{r}
# preprocesses data for use in prediction
X <- validation[,1:16]
Y <- validation[,17]

# Standardize Data
preProcessParams <- preProcess(X, method = c("center", "scale"))

# Transforms the validation test set to preprocessing parameters
transf.validation <- predict(preProcessParams, X)

# Validation training control parameters; creates one model based on hyperparameters passed
validationControl <- trainControl(method = "none",
                                  number = 0)
metric = "RMSE"
```

## SVM Validation
### SVM tuning parameters
```{r}
tunedSigma <- fit.svm$bestTune$sigma
tunedC <- fit.svm$bestTune$C
```

### SVM Validation Model Training and Prediction
Train model with tuned hyperparameters and predict on validation set
```{r}
# SVM Final Model
tuneGrid <- expand.grid(.sigma = tunedSigma,
                        .C = tunedC)

set.seed(seed)
svmModel <- train(totalstops~.,
                  data = training,
                  method = "svmRadial",
                  metric = metric,
                  preProcess= c("center", "scale"),
                  tuneGrid = tuneGrid,
                  trControl = validationControl)

#SVM Validation
set.seed(seed)
svmPredictions <- predict(svmModel, transf.validation)
svmRMSE <- RMSE(svmPredictions, validation$totalstops)
```

### SVM Validation Results
```{r}
svm.results <- list(baselineRMSE = min(fit.svm$results$RMSE),
                    validationRMSE = svmRMSE,
                    difference = min(fit.svm$results$RMSE) - svmRMSE)
svm.results
```


## XGBLinear Validation
### XGBLinear tuning Parameters
```{r}
# Set XGBlinear tuning parameters to global variable
xgbNrounds <- fit.xgbLinear$finalModel$tuneValue$nrounds
xgbLambda <- fit.xgbLinear$finalModel$tuneValue$lambda
xgbAlpha <- fit.xgbLinear$finalModel$tuneValue$alpha
xgbEta <- fit.xgbLinear$finalModel$tuneValue$eta
```

### XGBLinear Validation Model Training and Prediction
```{r}
tuneGrid <- expand.grid(.nrounds = xgbNrounds,
                        .lambda = xgbLambda,
                        .alpha = xgbAlpha,
                        .eta = xgbEta)

set.seed(seed)
xgbLinModel <- train(totalstops~.,
                  data = training,
                  method = "xgbLinear",
                  metric = metric,
                  preProcess= c("center", "scale"),
                  tuneGrid = tuneGrid,
                  trControl = validationControl)

# XGBLinear Validation
set.seed(seed)
xgbLinPredictions <- predict(xgbLinModel, transf.validation)
xgbLinRMSE <- RMSE(xgbLinPredictions, validation$totalstops)
```

### XGBLinear Validation Results
```{r}
xgbLinear.results <- list(baselineRMSE = min(fit.xgbLinear$results$RMSE),
                          validationRMSE = xgbLinRMSE,
                          difference = min(fit.xgbLinear$results$RMSE) - xgbLinRMSE)
xgbLinear.results
```


## RandomForest Validation Validation Model Training and Prediction
### RandomForest tuning Parameters
```{r}
# Set RandomForest tuning parameter to global variable
rfMtry <- fit.rf$bestTune$mtry
```

### RandomForest Validation Model Training and Prediction
```{r}
tuneGrid <- expand.grid(.mtrt = rfMtry)

set.seed(seed)
rfModel <- train(totalstops~.,
                 data = training,
                 method = "rf",
                 metric = metric,
                 preProcess= c("center", "scale"),
                 tuneGrid = tuneGrid,
                 trControl = validationControl)

# RandomForest Validation
set.seed(seed)
rfPredictions <- predict(rfModel, transf.validation)
rfRMSE <- RMSE(rfPredictions, validation$totalstops)
```

### RandomForest Validation Results
```{r}
rf.results <- list(baselineRMSE = min(fit.rf$results$RMSE),
                   validationRMSE = rfRMSE,
                   difference = min(fit.rf$results$RMSE) - rfRMSE)
rf.results
```



## Validation Comparison
```{r}
validationResults <- list('SVM' = c(svm.results),
                          'XGBLinear' = c(xgbLinear.results),
                          'RF' = c(rf.results))
as.data.frame(validationResults)


```


# Model Tuning
## SVM Tuning
## XGBLinear Tuning

## Random ForestTuning 

### Tuning mtry  
```{r}
library(parallel);library(doParallel)
#Grid Search Tuning
trainControl <- trainControl(method = "repeatedcv", 
                             number = 10, 
                             repeats = 3, 
                             search = "grid",
                             allowParallel = TRUE)
metric = "RMSE"
tunegrid <- expand.grid(.mtry = c(10:25))

#Initializes Parallel Processing
cluster <- makeCluster(detectCores()-1)
registerDoParallel(cluster)

#Fits model with mtry value 
set.seed(seed)
fit.rfGrid <- train(totalstops~.,
                 data = training,
                 method = "rf",
                 metric = metric,
                 preProcess = c("center", "scale"),
                 tuneGrid = tunegrid,
                 trControl = trainControl)

#Stops Parallel Processing
stopCluster(cluster)
registerDoSEQ()
```

### mtry tuning results
```{r}
fit.rfGrid
```

### Plot mtry tuning
```{r}
plot(fit.rfGrid)
```

Based on the pattern observed above, mtry values of 3:9 will be tested to determine 


### Tuning mtry using grid search2 
```{r}
#Grid Search Tuning
trainControl <- trainControl(method = "repeatedcv", 
                             number = 10, 
                             repeats = 3, 
                             search = "grid",
                             allowParallel = TRUE)
metric = "RMSE"
tunegrid <- expand.grid(.mtry = c(3:9))

#Initializes Parallel Processing
cluster <- makeCluster(detectCores()-1)
registerDoParallel(cluster)

#Fits model with mtry value 
set.seed(seed)
fit.rfGrid2 <- train(totalstops~.,
                 data = training,
                 method = "rf",
                 metric = metric,
                 preProc=c("center", "scale"),
                 tuneGrid = tunegrid,
                 trControl = trainControl)

#Stops Parallel Processing
stopCluster(cluster)
registerDoSEQ()
```

```{r}
plot(fit.rfGrid2)
```


### mtry Grid Search2 Results
```{r}
fit.rfGrid2
```


### Using optimal mtry value, search for optimal tree value
```{r}
trainControl <- trainControl(method="repeatedcv", 
                             number=10, 
                             repeats=3, 
                             search="grid")
tunegrid <- expand.grid(.mtry= 10)

#Initializes Parallel Processing
cluster <- makeCluster(detectCores()-1)
registerDoParallel(cluster)

modellist <- list()
for (ntree in c(1000, 1500, 2000, 2500)) {
  set.seed(seed)
  fit <- train(totalstops~., 
               data=training, 
               method="rf", 
               metric=metric, 
               tuneGrid=tunegrid,
               trControl=trainControl, 
               ntree=ntree)
  key <- toString(ntree)
  modellist[[key]] <- fit
}

#Stops Parallel Processing
stopCluster(cluster)
registerDoSEQ()
```

### ntree results
```{r}
ntree.results <- resamples(modellist)
summary(ntree.results)
```

```{r}
trainControl <- trainControl(method="repeatedcv", 
                             number=10, 
                             repeats=3, 
                             search="grid")
tunegrid <- expand.grid(.mtry= 9)

#Initializes Parallel Processing
cluster <- makeCluster(detectCores()-1)
registerDoParallel(cluster)

modellist2 <- list()
for (ntree in c(1700, 1900, 2100, 2200, 2300, 2500)) {
  set.seed(seed)
  fit <- train(totalstops~., 
               data=training, 
               method="rf", 
               metric=metric, 
               tuneGrid=tunegrid,
               trControl=trainControl, 
               ntree=ntree)
  key <- toString(ntree)
  modellist2[[key]] <- fit
}

#Stops Parallel Processing
stopCluster(cluster)
registerDoSEQ()

#results
ntree2.results <- resamples(modellist2)
summary(ntree2.results)
```


## Tuned mtry and ntree in Random Forest

```{r}
validationControl <- trainControl(method = "none",
                                  number = 0)
tuneGrid <- expand.grid(.mtry = 9)
metric = "rmse"

#Initialize Parallel Processing
cluster <- makeCluster(detectCores()-1)
registerDoParallel(cluster)

#Train model with tuned parameters
set.seed(seed)
rfModelTuned <- train(totalstops~.,
                  data = training,
                  method = "rf",
                  metric = metric,
                  preProcess= c("center", "scale"),
                  tuneGrid = tuneGrid,
                  ntree = 2300,
                  trControl = validationControl)

#Stops Parallel Processing
stopCluster(cluster)
registerDoSEQ()

set.seed(seed)
rfTunedPredictions <- predict(rfModelTuned, transf.validation)
rfRMSE <- RMSE(rfTunedPredictions, validation$totalstops)
rfRMSE

# # Generate the final model using the tuned hyperparamters, mtry and ntree
# finalModel <- randomForest(totalstops~.,
#                            data = training,
#                            mtry = 9,
#                            ntree = 2300)
```


Standardized RF RMSE: 127.6949
Validation RF RMSE: 346.2227
RMSE Difference: 218.5278
Tuned RF RMSE: 317.1614