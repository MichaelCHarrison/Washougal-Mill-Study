---
title: "R Notebook"
output: rmarkdown::github_document
---


# Initial Model Fit; without date feature

Load necessary package and data from file
```{r}
library(caret)
source("millstyleBreakdown.R")

# Data contains NAs generated from merging tables
data <- na.omit(millstyleBreakdown())
# Removing row identifier (millstyle), and initially date
data <- subset(data, select = -c(millstyle, date))
```


```{r}
#Establishes constant seed for consequent use in model development
seed <- 7
set.seed(seed)

# Partitions data to create training, validation, and testing sets for model comparison;
# Data is split 60/20/20 training, validation, testing
inVal <- createDataPartition(data$totalstops,
                             p = .6,
                             list = FALSE)
val_test <- data[-inVal,]
training <- data[inVal,]

inTrain <- createDataPartition(val_test$totalstops, 
                               p = .5,
                               list = FALSE)
validation <- val_test[inTrain,]
testing <- val_test[-inTrain,]
        
        
# Set training control parameters for repeated cross validation and evaluation metric
trainControl <- trainControl(method = "repeatedCV",
                             number = 10,
                             repeats = 3,
                             allowParallel = TRUE)
metric <- "RMSE"


# Prepares Workbook for Parallel Processing
library(parallel)
library(doParallel)
cluster <- makeCluster(detectCores()-1)
```


## Baseline Model Fit: No PreProcessing of Data
```{r}
# Prepares Workbook for Parallel Processing
library(parallel)
library(doParallel)
cluster <- makeCluster(detectCores()-1)
registerDoParallel(cluster)

#Linear Model
set.seed(seed)
fit.lm <- train(totalstops~.,
                data = training,
                method = "lm",
                metric = metric,
                trControl = trainControl)


#Generalized Linear Model
set.seed(seed)
fit.glm <- train(totalstops~.,
                 data = training,
                 method = "glm",
                 metric = metric,
                 trControl = trainControl)

#GLMNET
set.seed(seed)
fit.glmnet <- train(totalstops~.,
                    data = training,
                    method = "glmnet",
                    metric = metric,
                    trControl = trainControl)

# SVM
set.seed(seed)
fit.svm <- train(totalstops~.,
                 data = training,
                 method = "svmRadial",
                 metric = metric,
                 trControl = trainControl)

# CART
set.seed(seed)
grid <- expand.grid(.cp=c(0, 0.05, 0.1))
fit.cart <- train(totalstops~.,
                  data = training,
                  method = "rpart",
                  metric = metric,
                  tuneGrid=grid,
                  trControl = trainControl)
        

# KNN
set.seed(seed)
fit.knn <- train(totalstops~.,
                 data = training,
                 method = "knn",
                 metric = metric,
                 trControl = trainControl)

# Random Forest
set.seed(seed)
fit.rf <- train(totalstops~.,
                data = training,
                method = "rf",
                metric = metric,
                trControl = trainControl)

 
# Results
fit.results <- resamples(list(LM=fit.lm, 
                          GLM=fit.glm, 
                          GLMNET=fit.glmnet, 
                          SVM=fit.svm,
                          CART=fit.cart, 
                          KNN=fit.knn,
                          RF=fit.rf))

# Stops Parallel Processing
stopCluster(cluster)
registerDoSEQ()
```

### Baseline Model Fit results
```{r}
summary(fit.results)
```

### Baseline Model Fit results plot
```{r}
dotplot(fit.results)
```

### Baseline Model Fit Timings
```{r}
fit.results$timings
```


## Preprocessed Model Fit: Standardized
```{r}
# Initializes Parallel Processing
cluster <- makeCluster(detectCores()-1)
registerDoParallel(cluster)

#Linear Model

set.seed(seed)
standardized.lm <- train(totalstops~.,
                data = training,
                method = "lm",
                metric = metric,
                preProc=c("center", "scale"),
                trControl = trainControl)


#Generalized Linear Model
set.seed(seed)
standardized.glm <- train(totalstops~.,
                 data = training,
                 method = "glm",
                 metric = metric,
                 preProc=c("center", "scale"),
                 trControl = trainControl)

#GLMNET
set.seed(seed)
standardized.glmnet <- train(totalstops~.,
                    data = training,
                    method = "glmnet",
                    metric = metric,
                    preProc=c("center", "scale"),
                    trControl = trainControl)

# SVM
set.seed(seed)
standardized.svm <- train(totalstops~.,
                 data = training,
                 method = "svmRadial",
                 metric = metric,
                 preProc=c("center", "scale"),
                 trControl = trainControl)

# CART
set.seed(seed)
grid <- expand.grid(.cp=c(0, 0.05, 0.1))
standardized.cart <- train(totalstops~.,
                  data = training,
                  method = "rpart",
                  metric = metric,
                  preProc=c("center", "scale"),
                  tuneGrid=grid,
                  trControl = trainControl)
        

# KNN
set.seed(seed)
standardized.knn <- train(totalstops~.,
                 data = training,
                 method = "knn",
                 metric = metric,
                 preProc=c("center", "scale"),
                 trControl = trainControl)

# Random Forest
set.seed(seed)
standardized.rf <- train(totalstops~.,
                data = training,
                method = "rf",
                metric = metric,
                preProc=c("center", "scale"),
                trControl = trainControl)

 
# Results
standardized.results <- resamples(list(LM=standardized.lm, 
                                  GLM=standardized.glm, 
                                  GLMNET=standardized.glmnet, 
                                  SVM=standardized.svm,
                                  CART=standardized.cart, 
                                  KNN=standardized.knn,
                                  RF=standardized.rf))

#Stops Parallel Processing
stopCluster(cluster)
registerDoSEQ()
```

### Standardized Model Fit results
```{r}
summary(standardized.results)
```

### Standardized Model Fit results plot
```{r}
dotplot(standardized.results)
```
### Standardized Model Fit Timings
```{r}
standardized.results$timings
```




## Preprocessed Model Fit: YeoJohnson
```{r}
# Prepares Workbook for Parallel Processing and Initializes
library(parallel)
library(doParallel)
cluster <- makeCluster(detectCores()-1)
registerDoParallel(cluster)

#Linear Model

set.seed(seed)
yeojohnson.lm <- train(totalstops~.,
                data = training,
                method = "lm",
                metric = metric,
                preProc=c("YeoJohnson"),
                trControl = trainControl)


#Generalized Linear Model
set.seed(seed)
yeojohnson.glm <- train(totalstops~.,
                 data = training,
                 method = "glm",
                 metric = metric,
                 preProc=c("YeoJohnson"),
                 trControl = trainControl)

#GLMNET
set.seed(seed)
yeojohnson.glmnet <- train(totalstops~.,
                    data = training,
                    method = "glmnet",
                    metric = metric,
                    preProc=c("YeoJohnson"),
                    trControl = trainControl)

# SVM
set.seed(seed)
yeojohnson.svm <- train(totalstops~.,
                 data = training,
                 method = "svmRadial",
                 metric = metric,
                 preProc=c("YeoJohnson"),
                 trControl = trainControl)

# CART
set.seed(seed)
grid <- expand.grid(.cp=c(0, 0.05, 0.1))
yeojohnson.cart <- train(totalstops~.,
                  data = training,
                  method = "rpart",
                  metric = metric,
                  preProc=c("YeoJohnson"),
                  tuneGrid=grid,
                  trControl = trainControl)
        

# KNN
set.seed(seed)
yeojohnson.knn <- train(totalstops~.,
                 data = training,
                 method = "knn",
                 metric = metric,
                 preProc=c("YeoJohnson"),
                 trControl = trainControl)

# Random Forest
set.seed(seed)
yeojohnson.rf <- train(totalstops~.,
                data = training,
                method = "rf",
                metric = metric,
                preProc=c("YeoJohnson"),
                trControl = trainControl)

 
# Results
yeojohnson.results <- resamples(list(LM=yeojohnson.lm, 
                                  GLM=yeojohnson.glm, 
                                  GLMNET=yeojohnson.glmnet, 
                                  SVM=yeojohnson.svm,
                                  CART=yeojohnson.cart, 
                                  KNN=yeojohnson.knn,
                                  RF=yeojohnson.rf))

stopCluster(cluster)
registerDoSEQ()
```

### YeoJohnson Model Fit results
```{r}
summary(yeojohnson.results)
```

### YeoJohnson Model Fit results plot
```{r}
dotplot(yeojohnson.results)
```

### YeoJohnson Model Fit Timings
```{r}
yeojohnson.results$timings
```







## PreProcessed Model Fit: Standardized and YeoJohnson Transformation
```{r}
# Initializes Parallel Processing
cluster <- makeCluster(detectCores()-1)
registerDoParallel(cluster)

#Linear Model
set.seed(seed)
preProc.lm <- train(totalstops~.,
                data = training,
                method = "lm",
                metric = metric,
                preProc=c("center", "scale", "YeoJohnson"),
                trControl = trainControl)

#Generalized Linear Model
set.seed(seed)
preProc.glm <- train(totalstops~.,
                 data = training,
                 method = "glm",
                 metric = metric,
                 preProc=c("center", "scale", "YeoJohnson"),
                 trControl = trainControl)

#GLMNET
set.seed(seed)
preProc.glmnet <- train(totalstops~.,
                    data = training,
                    method = "glmnet",
                    metric = metric,
                    preProc=c("center", "scale", "YeoJohnson"),
                    trControl = trainControl)

# SVM
set.seed(seed)
preProc.svm <- train(totalstops~.,
                 data = training,
                 method = "svmRadial",
                 metric = metric,
                 preProc=c("center", "scale", "YeoJohnson"),
                 trControl = trainControl)

# CART
set.seed(seed)
grid <- expand.grid(.cp=c(0, 0.05, 0.1))
preProc.cart <- train(totalstops~.,
                  data = training,
                  method = "rpart",
                  metric = metric,
                  preProc=c("center", "scale", "YeoJohnson"),
                  tuneGrid=grid,
                  trControl = trainControl)
        
# KNN
set.seed(seed)
preProc.knn <- train(totalstops~.,
                 data = training,
                 method = "knn",
                 metric = metric,
                 preProc=c("center", "scale", "YeoJohnson"),
                 trControl = trainControl)

# Random Forest
set.seed(seed)
preProc.rf <- train(totalstops~.,
                data = training,
                method = "rf",
                metric = metric,
                preProc=c("center", "scale", "YeoJohnson"),
                trControl = trainControl)

 
# Results
preProc.results <- resamples(list(LM=preProc.lm, 
                                  GLM=preProc.glm, 
                                  GLMNET=preProc.glmnet, 
                                  SVM=preProc.svm,
                                  CART=preProc.cart, 
                                  KNN=preProc.knn,
                                  RF=preProc.rf))

stopCluster(cluster)
registerDoSEQ()
```

### PreProcessed Model Fit Results
```{r}
summary(preProc.results)
```

### PreProcessed Model Fit results plot
```{r}
dotplot(preProc.results)
```

### PreProcessed Model Fit Timings
```{r}
preProc.results$timings
```






### PreProcessed Model results timing
```{r}
preProc.results$timings
```

## Gradient Boosting Algorithms Compared to Random Forest

```{r}
# Initializes Parallel Processing
cluster <- makeCluster(detectCores()-1)
registerDoParallel(cluster)

# Gradient Boosted Machine
set.seed(seed)
preProc.gbm <- train(totalstops~.,
                 data = training,
                 method = "gbm",
                 metric = metric,
                 preProc=c("center", "scale", "YeoJohnson"),
                 trControl = trainControl)

# XGBLinear
set.seed(seed)
preProc.xgbLinear <- train(totalstops~.,
                 data = training,
                 method = "xgbLinear",
                 metric = metric,
                 preProc=c("center", "scale", "YeoJohnson"),
                 trControl = trainControl)

# XGBTree
set.seed(seed)
preProc.xgbTree <- train(totalstops~.,
                 data = training,
                 method = "xgbTree",
                 metric = metric,
                 preProc=c("center", "scale", "YeoJohnson"),
                 trControl = trainControl)

#Stops Parallel Processing
stopCluster(cluster)
registerDoSEQ()

# Results
preproc.boosting.results <- resamples(list(GBM = preProc.gbm,
                                          XGBLinear = preProc.xgbLinear,
                                          XGBTree = preProc.xgbTree,
                                          RF = preProc.rf))
```

### Boosting Algorithm Results
```{r}
summary(preproc.boosting.results)
```

# Algorithms Selected For Validation

- SVM
- GBM
- XGBLinear
- RandomForest

## Validation Set
```{r}
# preprocesses data for use in prediction
X <- validation[,1:16]
Y <- validation[,17]

preProcessParams <- preProcess(X, method = c("center", "scale"))
# transforms the validation test set to preprocessing parameters
transf.validation <- predict(preProcessParams, X)
```

## SVM Validation

```{r}
preProc.svm$finalModel
```
```{r}
preProc.svm$bestTune
```



```{r}
# SVM Final Model

svmControl <- trainControl(method = "none",
                           number = 0,
                           repeats = 0,
                           allowParallel = TRUE)
metric = "rmse"

tuneGrid <- expand.grid(.sigma = 0.01334993,
                        .C = 1)

svmModel <- train(totalstops~.,
                  data = training,
                  method = "svmRadial",
                  metric = metric,
                  tuneGrid = tuneGrid,
                  trControl = svmControl)


# library(kernlab)
# svmModel <- lssvm(x = totalstops~.,
#                   data = validation,
#                   y = validation$totalstops)

#SVM Validation
svmPredictions <- predict(svmModel, transf.validation)
svmRMSE <- RMSE(svmPredictions, validation$totalstops)
```

### SVM Validation RMSE
```{r}
svmRMSE
```


## GBM Validation

```{r}
preProc.gbm$finalModel
```

```{r}
# GBM Final Model
gbmModel <- preProc.gbm$finalModel

# GBM Validation 
gbmPredictions <- predict(gbmModel, transf.validation)
gbmRMSE <- RMSE(gbmPredictions, validation$totalstops)
```

## XGBLinear Validation

```{r}
preProc.xgbLinear$finalModel
```

```{r}
xgbLinModel <- preProc.xgbLinear$finalModel

xgbLinPredictions <- predict(xgbLinModel, transf.validation)
xgbRMSE <- RMSE(xgbLinPredictions, validation$totalstops)
```

## RandomForest Validation
```{r}
preProc.rf$finalModel
```


```{r}
rfModel <- preProc.rf$finalModel

rfPredictions <- predict(rfModel, transf.validation)
rfRMSE <- RMSE(rfPredictions, validation$totalstops)
```


# SVM Tuning

# GBM Tuning

# XGBLinear Tuning

# Random ForestTuning 

```{r}
preProc.rf$bestTune
```

### Tuning mtry  
```{r}
#Grid Search Tuning
trainControl <- trainControl(method = "repeatedcv", 
                             number = 10, 
                             repeats = 3, 
                             search = "grid",
                             allowParallel = TRUE)
metric = "RMSE"
tunegrid <- expand.grid(.mtry = c(10:25))

#Initializes Parallel Processing
cluster <- makeCluster(detectCores()-1)
registerDoParallel(cluster)

#Fits model with mtry value 
set.seed(seed)
fit.rfGrid <- train(totalstops~.,
                 data = training,
                 method = "rf",
                 metric = metric,
                 preProc=c("center", "scale", "YeoJohnson"),
                 tuneGrid = tunegrid,
                 trControl = trainControl)

#Stops Parallel Processing
stopCluster(cluster)
registerDoSEQ()
```

### mtry tuning results
```{r}
fit.rfGrid
```

### Plot mtry tuning
```{r}
plot(fit.rfGrid)
```

Based on the pattern observed above, mtry values of 3:9 will be tested to determine 


### Tuning mtry using grid search2 
```{r}
#Grid Search Tuning
trainControl <- trainControl(method = "repeatedcv", 
                             number = 10, 
                             repeats = 3, 
                             search = "grid",
                             allowParallel = TRUE)
metric = "RMSE"
tunegrid <- expand.grid(.mtry = c(3:9))

#Initializes Parallel Processing
cluster <- makeCluster(detectCores()-1)
registerDoParallel(cluster)

#Fits model with mtry value 
set.seed(seed)
fit.rfGrid2 <- train(totalstops~.,
                 data = training,
                 method = "rf",
                 metric = metric,
                 preProc=c("center", "scale", "YeoJohnson"),
                 tuneGrid = tunegrid,
                 trControl = trainControl)

#Stops Parallel Processing
stopCluster(cluster)
registerDoSEQ()
```


### mtry Grid Search2 Results
```{r}
fit.rfGrid2
```


### Using optimal mtry value, search for optimal tree value
```{r}
trainControl <- trainControl(method="repeatedcv", 
                             number=10, 
                             repeats=3, 
                             search="grid")
tunegrid <- expand.grid(.mtry= 9)

#Initializes Parallel Processing
cluster <- makeCluster(detectCores()-1)
registerDoParallel(cluster)

modellist <- list()
for (ntree in c(1000, 1500, 2000, 2500)) {
  set.seed(seed)
  fit <- train(totalstops~., 
               data=training, 
               method="rf", 
               metric=metric, 
               tuneGrid=tunegrid,
               trControl=trainControl, 
               ntree=ntree)
  key <- toString(ntree)
  modellist[[key]] <- fit
}

#Stops Parallel Processing
stopCluster(cluster)
registerDoSEQ()
```

### ntree results
```{r}
ntree.results <- resamples(modellist)
summary(ntree.results)
```

```{r}
trainControl <- trainControl(method="repeatedcv", 
                             number=10, 
                             repeats=3, 
                             search="grid")
tunegrid <- expand.grid(.mtry= 9)

#Initializes Parallel Processing
cluster <- makeCluster(detectCores()-1)
registerDoParallel(cluster)

modellist2 <- list()
for (ntree in c(1700, 1900, 2100, 2300)) {
  set.seed(seed)
  fit <- train(totalstops~., 
               data=training, 
               method="rf", 
               metric=metric, 
               tuneGrid=tunegrid,
               trControl=trainControl, 
               ntree=ntree)
  key <- toString(ntree)
  modellist2[[key]] <- fit
}

#Stops Parallel Processing
stopCluster(cluster)
registerDoSEQ()

#results
ntree2.results <- resamples(modellist2)
summary(ntree2.results)
```


## Tuned mtry and ntree in Random Forest

```{r}
# Generate the final model using the tuned hyperparamters, mtry and ntree
finalModel <- randomForest(totalstops~.,
                           data = training,
                           mtry = 2,
                           ntree = 1900)

# Need to separate the features from target before running preProcess()
preProccessParams <- preProcess(validation[,1:17], 
                                method = c("center", "scale", "YeoJohnson"))

# Transform the validation set using the parameters
transf.validation <- predict(preprocessParmas, validation[,1:17])

# Make predictions on validation set
finalPredictions <- predict(finalModel, transf.validation)
rfRMSE <- RMSE(finalPredictions, validation$totalstops)
```



